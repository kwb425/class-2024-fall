{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kwb425/class-2024-fall/blob/main/class-2024-fall_1206-0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_QBPnrEWzH9"
   },
   "outputs": [],
   "source": [
    "!pip install gradio\n",
    "!pip install SpeechRecognition\n",
    "!pip install gtts\n",
    "from PIL import Image\n",
    "from gtts import gTTS\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import gradio as gr\n",
    "import soundfile as sf\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill mask\n",
    "fill_mask_model = pipeline(model=\"bert-base-uncased\")\n",
    "\n",
    "def fill_mask(text):\n",
    "    results = fill_mask_model(text)\n",
    "    results_list = [{ 'score': result['score'], 'token_str': result['token_str'], 'sequence': result['sequence']} for result in results]\n",
    "    return results_list\n",
    "\n",
    "interface = gr.Interface(fn=fill_mask, inputs=\"text\", outputs=\"json\")\n",
    "interface.launch()\n",
    "\n",
    "# The quick brown fox jumps over the deep [MASK].\n",
    "# In the center of the city, you can find busy [MASK].\n",
    "# The solar system consists of eight planets, including Earth and [MASK]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image explanation\n",
    "caption_generator = pipeline(model=\"ydshieh/vit-gpt2-coco-en\")\n",
    "\n",
    "def generate_caption(img):\n",
    "    pil_img = Image.fromarray(img)\n",
    "    result = caption_generator(pil_img)[0]['generated_text']\n",
    "    return result\n",
    "\n",
    "interface = gr.Interface(fn=generate_caption, inputs=\"image\", outputs=\"text\")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech-to-Text (STT)\n",
    "def stt(audio):\n",
    "    stt_model = sr.Recognizer()\n",
    "    s_rate, audio_data = audio\n",
    "    sf.write(\"stt_input.wav\", audio_data, s_rate)\n",
    "\n",
    "    with sr.AudioFile(\"stt_input.wav\") as source:\n",
    "        audio_recorded = stt_model.record(source)\n",
    "        text = stt_model.recognize_google(audio_recorded)\n",
    "    return text\n",
    "\n",
    "interface = gr.Interface(fn=stt, inputs=\"microphone\", outputs=\"text\")\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text-to-Speech (TTS)\n",
    "def tts(text):\n",
    "  tts_model = gTTS(text)\n",
    "  tts_model.save(\"tts_output.wav\")\n",
    "  return \"tts_output.wav\"\n",
    "\n",
    "interface = gr.Interface(fn=tts, inputs=\"text\", outputs=\"audio\")\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Voice Chatbot\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "def stt(audio):\n",
    "  stt_model = sr.Recognizer()\n",
    "  s_rate, audio_data = audio\n",
    "  sf.write(\"stt_input.wav\", audio_data, s_rate)\n",
    "\n",
    "  with sr.AudioFile(\"stt_input.wav\") as source:\n",
    "    audio_recorded = stt_model.record(source)\n",
    "    text = stt_model.recognize_google(audio_recorded)\n",
    "  return text\n",
    "\n",
    "def tts(text):\n",
    "  tts_model = gTTS(text)\n",
    "  tts_model.save(\"tts_output.wav\")\n",
    "  return \"tts_output.wav\"\n",
    "\n",
    "# No chat history\n",
    "def chatbot(text):\n",
    "  input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "  output_ids = chat_model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "  output = tokenizer.decode(output_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "  return output\n",
    "\n",
    "def voice_chatbot(audio):\n",
    "  user_text = stt(audio)\n",
    "  chatbot_text = chatbot(user_text)\n",
    "  speech_file = tts(chatbot_text)\n",
    "  return user_text, chatbot_text, speech_file\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=voice_chatbot,\n",
    "    inputs=\"microphone\",\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"User Text\"),\n",
    "        gr.Textbox(label=\"Chatbot Response\"),\n",
    "        gr.Audio(label=\"Chatbot Audio\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUR6YB4o85f8Gwntxny0cj",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
